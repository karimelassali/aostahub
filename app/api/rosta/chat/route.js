import { google } from '@ai-sdk/google';
import { generateText } from 'ai';
import { NextResponse } from 'next/server';

let chatHistory = [];

/**
 * Updates the chat history with a new message and maintains a maximum of 10 messages.
 *
 * @param {string} role - The role of the message sender (e.g., "user" or "bot").
 * @param {string} content - The content of the message to be added to chat history.
 */
async function updateChatHistory(role, content) {
    chatHistory.push({ role, content });
    if (chatHistory.length > 20) {
        chatHistory = chatHistory.slice(chatHistory.length - 10); // Keep last 10 messages
    }
}

/**
 * Handles incoming POST requests to the /api/rosta/chat endpoint.
 *
 * Request body must contain a JSON object with the following properties:
 * - `prompt`: The user's input to be sent to the AI model.
 * - `imgPrompt`: The URL of the image to be sent to the AI model, if any.
 *
 * The endpoint returns a JSON response with the following properties:
 * - `response`: The text generated by the AI model.
 * - `error`: An error message if there was an error generating text.
 *
 * @param {NextApiRequest} req
 * @returns {Promise<NextApiResponse>}
 */
export async function POST(req) {
    const { messages } = await req.json();
    const lastMessage = messages[messages.length - 1];
    const prompt = lastMessage.content;

    const content = [
        {
            type: 'text',
            text: `
                You are Rosta, an AI assistant for **Aosta Hub**. Your role is to help users in **Aosta city** with questions, promoting interaction and community. Respond concisely, in the same language, with a friendly tone. Always refer to yourself as **Rosta**.
                History: ${JSON.stringify(chatHistory)}
                Request: ${prompt}
            `,
        },
    ];

    try {
        const response = await generateText({
            model: google('gemini-1.5-flash'),
            messages: [{ role: 'user', content }],
        });

        if (response) {
            await Promise.all([
                updateChatHistory('user', prompt),
                updateChatHistory('assistant', response.text)
            ]);
        }

        return NextResponse.json({ response: response.text });
    } catch (error) {
        console.error("Error processing chat:", error);
        return NextResponse.json(
            { error: "Failed to process message" },
            { status: 500 }
        );
    }
}

// Create an OpenAI API client (that's edge friendly!)
// const openai = new OpenAI({
//   apiKey: process.env.OPENAI_API_KEY,
// });

// IMPORTANT! Set the runtime to edge
export const runtime = 'edge';
